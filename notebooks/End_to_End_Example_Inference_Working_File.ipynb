{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92111249",
   "metadata": {},
   "source": [
    "# End to End Inference Tutorial\n",
    "Here we implement a complete end-to-end use of paltas. This notebook is intended as a 'minimal reproducible example', and thus doesn't use the full extent of the package, but should be a useful starting point. \\\n",
    "A number of the code-blocks simply run command-line instructions. This is intentional, as paltas is designed to run in the command-line. Furthermore, running such command-line statements allows easier transfer to remote computing clusters/parallelisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50353929",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "1. To be able to implement a simple end-to-end example of Paltas\n",
    "2. To understand how each of the packages inter-communicate, and which packages need to be run (and when), to perform hierarchichal inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f3a6c-5fb2-41de-8c0a-02004fa5e3da",
   "metadata": {},
   "source": [
    "# Import Packages\n",
    "Here we import the required packages and define the training and model directories (where the training images and model weights are stored, respectively).\\\n",
    "The '/home/runner/work' referred to here is required to run this notebook as a Github Action, but should be changed to a prefered directory when running this notebook locally.\\\n",
    "Although tensorflow, emcee and ipython do not form part of the requirements for paltas, they are required for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16173c23-7349-4a92-b77b-a3137d750555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T13:40:43.642700Z",
     "start_time": "2023-11-14T13:38:50.700595Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#paltas_directory = './'\n",
    "#training_directory = '/home/runner/work/notebooks/End_to_End_Tutorial_Files/' #For github actions\n",
    "#model_directory = '/home/runner/work/notebooks/End_to_End_Tutorial_Files/'\n",
    "#training_directory = '/global/u2/p/phil1884/paltas/notebooks/End_to_End_Tutorial_Files' #For NERSC\n",
    "#model_directory = '/global/u2/p/phil1884/paltas/notebooks/End_to_End_Tutorial_Files'\n",
    "#paltas_directory = '/global/u2/p/phil1884/paltas/'\n",
    "training_directory = '/mnt/extraspace/hollowayp/paltas_data/Example_SL_12/' #For Glamdring\n",
    "model_directory = '/mnt/extraspace/hollowayp/paltas_data/Example_SL_12/'\n",
    "paltas_directory = '/mnt/zfsusers/hollowayp/paltas/'\n",
    "import os\n",
    "os.chdir(paltas_directory)\n",
    "from paltas.Analysis import hierarchical_inference,dataset_generation, loss_functions, conv_models\n",
    "from IPython.display import display,Pretty\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as pl\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import corner\n",
    "import emcee\n",
    "import numba\n",
    "import h5py\n",
    "import glob\n",
    "import sys\n",
    "import datetime\n",
    "random_seed = 4\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df3344",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T14:11:26.084542Z",
     "start_time": "2023-11-14T14:11:26.080914Z"
    }
   },
   "outputs": [],
   "source": [
    "#!python3 -m pip install arviz --upgrade\n",
    "import arviz as az\n",
    "az.style.use(\"arviz-plasmish\")\n",
    "#az.style.available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff917398",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298f9d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T13:38:41.005515Z",
     "start_time": "2023-11-14T13:38:39.618014Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "paltas_directory = '/mnt/zfsusers/hollowayp/paltas/'\n",
    "os.chdir(paltas_directory)\n",
    "dill.load_session('/mnt/zfsusers/hollowayp/paltas/notebooks/End_to_End_Example_Inference_Working_File.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de703af",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Save Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66eada3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "#dill.dump_session('/mnt/zfsusers/hollowayp/paltas/notebooks/End_to_End_Example_Inference_Working_File.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5059aa-ca43-4cc6-b846-0d12d189b87e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Generate Images\n",
    "We start by generating lensed images divided into training and validation sets. The images are saved within one h5 file for each run of generate.py. We'll first look at the configuration file used to determine the properties of the generated images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741f6a4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display(Pretty(glob.glob(f'{training_directory}/training/1/config*')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da3c0c2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We then run the image generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d5184-2a2b-4fdc-a8e0-660b3996f105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T16:51:11.772142Z",
     "start_time": "2023-08-07T16:50:01.191Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 ./paltas/generate.py ./paltas/Configs/Examples/config_simple_tutorial.py /$training_directory/training/1 --n 100 --tf_record --h5\n",
    "!python3 ./paltas/generate.py ./paltas/Configs/Examples/config_simple_tutorial.py /$training_directory/validation/1 --n 100 --tf_record --h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01e41a",
   "metadata": {},
   "source": [
    "# Plot the Generated Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506f479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T15:35:30.933100Z",
     "start_time": "2023-11-14T15:35:25.822667Z"
    }
   },
   "outputs": [],
   "source": [
    "train_folders_list = glob.glob(f'{training_directory}/training/**/metadata.csv',recursive=True)\n",
    "for ii,m_i in tqdm(enumerate(train_folders_list)):\n",
    "    if ii==0:\n",
    "        generated_image_params_db = pd.read_csv(f'{train_folders_list[ii]}')\n",
    "    else:\n",
    "        generated_image_params_db = pd.concat([generated_image_params_db,\n",
    "                                pd.read_csv(f'{train_folders_list[ii]}')],ignore_index=True)\n",
    "\n",
    "try:\n",
    "    generated_image_params_db=generated_image_params_db.drop(['cosmology_parameters_cosmology_name',\n",
    "                                        'detector_parameters_background_noise',\n",
    "                                        'detector_parameters_exposure_time',\n",
    "                                        'detector_parameters_magnitude_zero_point',\n",
    "                                        'detector_parameters_num_exposures',\n",
    "                                        'detector_parameters_pixel_scale',\n",
    "                                        'detector_parameters_read_noise',\n",
    "                                        'detector_parameters_sky_brightness',\n",
    "                                        'lens_light_parameters_output_ab_zeropoint',\n",
    "                                        'main_deflector_parameters_M200',\n",
    "                                        'detector_parameters_ccd_gain',\n",
    "                                        'psf_parameters_fwhm',\n",
    "                                        'psf_parameters_psf_type',\n",
    "                                        'seed',\n",
    "                                        'source_parameters_output_ab_zeropoint',\n",
    "                                        'main_deflector_parameters_dec_0',\n",
    "                                        'main_deflector_parameters_ra_0',\n",
    "                                        'source_parameters_n_sersic',\n",
    "                                        'lens_light_parameters_n_sersic'],axis=1)\n",
    "except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32f560",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T15:36:58.949324Z",
     "start_time": "2023-11-14T15:36:52.422691Z"
    }
   },
   "outputs": [],
   "source": [
    "#corner.corner(generated_image_params_db,labels=generated_image_params_db.columns)\n",
    "corner.corner(generated_image_params_db[learning_params],\n",
    "              labels=[elem.replace('main_deflector_parameters_','') for elem in learning_params])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ab7d0-2334-4cb7-9e77-a8f656950667",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train Model\n",
    "The neural network is then trained. The --h5 indicates that the images were originally saved as h5 files, and should be retrieved as such. Again, most of the work is done by the configration file, so we'll inspect that first. We define the learning parameters (the lens properties the network should determine), in this file - we chose the Einstein radius, shear, power-law slope, position and ellipticity in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2039a",
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Pretty(\"./paltas/Analysis/AnalysisConfigs/train_config_examp_tutorial.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db128f92-4fa7-4a30-9f0d-ccfa873e3705",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 ./paltas/Analysis/train_model.py ./paltas/Analysis/AnalysisConfigs/train_config_examp_tutorial.py --h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab7275b-80e1-40d3-84ea-21c0030062ec",
   "metadata": {},
   "source": [
    "# Generate Model Predictions\n",
    "Having trained the model, we locate the filename of the final epoch (this can be hardcoded instead if desired)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb92a00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T13:40:54.505795Z",
     "start_time": "2023-11-14T13:40:54.143044Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model_weights_list(directory):\n",
    "    \"\"\" Function to return a list of weights filenames from the network\n",
    "    args: Directory containing the training, validation and weights files \"\"\"\n",
    "    weights_list = glob.glob(f'{directory}/model_weights/*')\n",
    "    weights_list = [elem.split('model_weights/')[1] for elem in weights_list]\n",
    "    return weights_list\n",
    "\n",
    "def return_final_epoch_weights(directory):\n",
    "    \"\"\" File to return the weight filename of the final trained epoch\n",
    "    args: Directory containing the training, validation and weights files \"\"\"\n",
    "    weights_list = load_model_weights_list(directory)\n",
    "    print(weights_list)\n",
    "    final_epoch =  np.max([int(elem.split('-')[0]) for elem in weights_list])\n",
    "    w_filename = [x for x in weights_list if x.startswith(\"{:02d}\".format(final_epoch)+'-')][0]\n",
    "    print('FINAL EPOCH',w_filename)\n",
    "    return directory+'/model_weights/'+w_filename\n",
    "\n",
    "def return_list_of_weight_files(directory):\n",
    "    '''Returns list of weight files, ordered by their creation date'''\n",
    "    files = list(filter(os.path.isfile, glob.glob(f'{directory}/model_weights/*h5')))\n",
    "    files.sort(key=lambda x: os.path.getmtime(x))\n",
    "    return files\n",
    "\n",
    "final_weights_filename = return_final_epoch_weights(model_directory)\n",
    "\n",
    "def retrieve_training_prior():\n",
    "    prior_path = glob.glob(f'{training_directory}/**/norm*',recursive=True)[0]\n",
    "    print(f'Retrieving prior path from {prior_path}')\n",
    "    training_prior_db = pd.read_csv(prior_path)\n",
    "    return training_prior_db\n",
    "\n",
    "prior_db = retrieve_training_prior()\n",
    "prior_db_indx = prior_db.set_index(prior_db['parameter'])\n",
    "print('NOTE: This training prior should really encompass all the training images, not just a subset (i.e. not just one folder of them)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4feb9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(prior_db['mean']))\n",
    "print(list(prior_db['std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69745512-b912-4e77-b50d-ef31ab3d3a7f",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "The trained model is loaded (along with the network weights from the final epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4b3b0-0cad-498e-b9e4-f3654f595592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T13:41:14.275681Z",
     "start_time": "2023-11-14T13:40:58.798042Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(model_weights_filename,loss_type,model_type,learning_params,log_learning_params,img_size):\n",
    "    \"\"\" Loads the trained model\n",
    "    args: \n",
    "    model_weights_filename (str): .h5 file containing the weights of the trained model.\n",
    "    loss_type (str): 'full' or 'diag', depending on the type of covariance matrix chosen\n",
    "    model type (str): 'xresnet34' or 'xresnet101', according to the choice of network\n",
    "    learning_params (list of str): Parameters learnt by the network\n",
    "    img_size (int): Dimensions of the input images\"\"\"\n",
    "    num_params = len(learning_params+log_learning_params)\n",
    "    if loss_type == 'full':\n",
    "        num_outputs = num_params + int(num_params*(num_params+1)/2)\n",
    "        loss_func = loss_functions.FullCovarianceLoss(num_params)\n",
    "    elif loss_type == 'diag':\n",
    "        num_outputs = 2*num_params\n",
    "        loss_func = loss_functions.DiagonalCovarianceLoss(num_params)\n",
    "    if model_type == 'xresnet101':\n",
    "        model = conv_models.build_xresnet101(img_size,num_outputs)\n",
    "    if model_type == 'xresnet34':\n",
    "        model = conv_models.build_xresnet34(img_size,num_outputs)\n",
    "    model.load_weights(model_weights_filename,by_name=True,skip_mismatch=True)\n",
    "    return model,loss_func,num_params\n",
    "\n",
    "#Import training configs\n",
    "print(\"NB: Need to make sure the following is importing the correct training configuration file: Currently loading train_config_Simpipeline\")\n",
    "from paltas.Analysis.AnalysisConfigs.train_config_Simpipeline import learning_params,batch_size,flip_pairs,\\\n",
    "                                                               n_epochs,random_seed,norm_images,\\\n",
    "                                                               loss_function,model_type,\\\n",
    "                                                               npy_folders_train,img_size\n",
    "\n",
    "\n",
    "corner_param_print= [elem.replace('main_deflector_parameters_','').replace('subhalo_parameters_','').\\\n",
    "                     replace('theta','\\Theta').replace('gamma','\\gamma') for elem in learning_params]\n",
    "\n",
    "model_dict = {}\n",
    "for ii,epoch_i_weights in tqdm(enumerate([return_list_of_weight_files(model_directory)[-1]])): #Just getting the most recent epoch\n",
    "    if ii==0:\n",
    "        model,loss_func,num_params = load_model(epoch_i_weights,loss_function,learning_params=learning_params,\\\n",
    "                             log_learning_params=[],model_type=model_type,img_size=img_size)\n",
    "        model_dict[ii]=model\n",
    "    else: \n",
    "        model,_,_ = load_model(epoch_i_weights,loss_function,learning_params=learning_params,\\\n",
    "                        log_learning_params=[],model_type=model_type,img_size=img_size)\n",
    "        model_dict[ii]=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_list_of_weight_files(model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486bfe70-a67b-4273-ae2c-b57ebf79be5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate Network Predictions\n",
    "The network predictions are then loaded, for testing on the validation set generated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649580d4-d101-4350-b992-a98f2ef3565c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T13:41:21.054244Z",
     "start_time": "2023-11-14T13:41:21.040299Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_network_predictions(test_folder,norm_path,learning_params,log_learning_params,loss_type,\n",
    "                            loss_func,model,shuffle=True,\n",
    "                            norm_images=True,log_norm_images=False):\n",
    "    \"\"\"\n",
    "    Generate neural network predictions given a paltas generated folder of images\n",
    "\n",
    "    Args:\n",
    "        test_folder (string): Path to folder of paltas generated images, \n",
    "            containig a data.tfrecord file\n",
    "        norm_path (string): Path to .csv containing normalization of parameters\n",
    "            applied during training of network\n",
    "        learning_params (list(string)): Names of parameters learned\n",
    "        loss_type (string): only 'diag' currently supported for this notebook\n",
    "        loss_func (paltas.Analysis.loss_function): Loss function object, (needs\n",
    "            draw_samples() and convert_output() functionality)\n",
    "        model (paltas.Analysis.conv_models): Trained neural network with weights\n",
    "            loaded\n",
    "        shuffle (bool, default=True): If True, the order of the test set is shuffled\n",
    "            when generating predictions\n",
    "        norm_images (bool, default=True): If True, normalize test set images\n",
    "        log_norm_images (bool, default=False): If True, test set imags are\n",
    "            log-normalized and rescaled to range (0,1)\n",
    "\n",
    "    Returns:\n",
    "        y_test, y_pred, std_pred, prec_pred\n",
    "    \"\"\"\n",
    "\n",
    "    tfr_test_path = os.path.join(test_folder,'data.tfrecord')\n",
    "    input_norm_path = norm_path\n",
    "    #The following code implementation here and in the hierarchical inference function below assumes a diagonal covariance matrix\n",
    "    if loss_type !='diag':\n",
    "        raise ValueError('loss_type not supported in this notebook')\n",
    "    tf_dataset_test = dataset_generation.generate_tf_dataset(tf_record_path = tfr_test_path,\\\n",
    "                                                             learning_params = learning_params,\n",
    "                                                             batch_size = 3,\\\n",
    "                                                             n_epochs = 1,\\\n",
    "                                                             norm_images=norm_images,\n",
    "                                                             kwargs_detector=None,\\\n",
    "                                                             input_norm_path=input_norm_path,\n",
    "                                                             log_learning_params=log_learning_params,\\\n",
    "                                                             shuffle=shuffle)\n",
    "\n",
    "    y_test_list = [];y_pred_list = []\n",
    "    std_pred_list = [];cov_pred_list = []\n",
    "    predict_samps_list = []\n",
    "\n",
    "    for batch in tf_dataset_test:\n",
    "        images = batch[0].numpy()\n",
    "        y_test = batch[1].numpy()\n",
    "        \n",
    "        # use unrotated output for covariance matrix\n",
    "        output = model.predict(images)\n",
    "        y_pred, log_var_pred = loss_func.convert_output(output)\n",
    "\n",
    "        # compute std. dev.\n",
    "        std_pred = np.exp(log_var_pred/2)\n",
    "        cov_mat = np.empty((len(std_pred),len(std_pred[0]),len(std_pred[0])))\n",
    "        for i in range(len(std_pred)):\n",
    "            cov_mat[i] = np.diag(std_pred[i]**2)\n",
    "\n",
    "        y_test_list.append(y_test)\n",
    "        y_pred_list.append(y_pred)\n",
    "        std_pred_list.append(std_pred)\n",
    "        cov_pred_list.append(cov_mat)\n",
    "\n",
    "    y_test = np.concatenate(y_test_list)\n",
    "    y_pred = np.concatenate(y_pred_list)\n",
    "    std_pred = np.concatenate(std_pred_list)\n",
    "    cov_pred = np.concatenate(cov_pred_list)\n",
    "\n",
    "    if input_norm_path is not None:\n",
    "        dataset_generation.unnormalize_outputs(input_norm_path,learning_params+log_learning_params,\n",
    "                                        y_pred,standard_dev=std_pred,cov_mat=cov_pred)\n",
    "        dataset_generation.unnormalize_outputs(input_norm_path,learning_params+log_learning_params,\n",
    "                                        y_test)\n",
    "    prec_pred = np.linalg.inv(cov_pred)\n",
    "   \n",
    "    return y_test, y_pred, std_pred, prec_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f31b65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:46:22.301564Z",
     "start_time": "2023-11-14T18:46:22.296465Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7920e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T13:42:19.855865Z",
     "start_time": "2023-11-14T13:41:22.156841Z"
    }
   },
   "outputs": [],
   "source": [
    "network_predictions_dict = {}\n",
    "key_indx = np.linspace(0,len(model_dict.keys())-1,2).astype('int') #Just retrieving 2 epochs, including the last. \n",
    "for epoch_i in tqdm(np.array(list(model_dict.keys()))[key_indx]):\n",
    "     network_predictions_dict[epoch_i] = gen_network_predictions(\\\n",
    "                        test_folder=training_directory+'/validation/1',\\\n",
    "                        norm_path=glob.glob(f'{training_directory}/**/norm*',recursive=True)[0],\n",
    "                        #norm_path = training_directory+'/training/1/norms.csv',\\\n",
    "                        learning_params=learning_params,\\\n",
    "                        log_learning_params = [],\\\n",
    "                        loss_type=loss_function,\n",
    "                        loss_func=loss_func,\\\n",
    "                        model=model_dict[epoch_i],\n",
    "                        shuffle=False, #NOT shuffling here, so the network outputs can be compared with other parameters in the test set.\n",
    "                        norm_images=norm_images,\n",
    "                        log_norm_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d38adc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T13:42:51.918898Z",
     "start_time": "2023-11-14T13:42:51.908630Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde264e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T14:12:32.946508Z",
     "start_time": "2023-11-14T14:11:33.673279Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plot to see what proportion of the network posteriors (i.e the network outputs) lie outside the training prior, i.e to see how many the network believes it is \n",
    "#extrapolating for.\n",
    "\n",
    "X_plot_dict = {'main_deflector_parameters_theta_E':np.linspace(-3,3,100),\n",
    "'main_deflector_parameters_gamma1':np.linspace(-0.5,0.5,100),\n",
    "'main_deflector_parameters_gamma2':np.linspace(-0.5,0.5,100),\n",
    "'main_deflector_parameters_gamma':np.linspace(0,3,100),\n",
    "'main_deflector_parameters_e1':np.linspace(-0.5,0.5,100),\n",
    "'main_deflector_parameters_e2':np.linspace(-0.5,0.5,100),\n",
    "'main_deflector_parameters_center_x':np.linspace(-0.5,0.5,100),\n",
    "'main_deflector_parameters_center_y':np.linspace(-0.5,0.5,100),\n",
    "}\n",
    "\n",
    "N_cols=4\n",
    "fig,ax = pl.subplots(2,N_cols,figsize=(20,10))\n",
    "fig2,ax2 = pl.subplots(1,figsize=(8,5))\n",
    "for n_i,p_i in enumerate(prior_db['parameter']):\n",
    "    x = n_i%N_cols\n",
    "    y = np.floor(n_i/N_cols).astype('int')\n",
    "    outside_prior = (network_pred_mu_db[p_i].to_numpy()<(prior_db_indx.loc[p_i,'mean']-prior_db_indx.loc[p_i,'std'])) |\\\n",
    "                    (network_pred_mu_db[p_i].to_numpy()>(prior_db_indx.loc[p_i,'mean']+prior_db_indx.loc[p_i,'std']))\n",
    "    ax[y,x].plot(X_plot_dict[p_i],norm.pdf(np.array([X_plot_dict[p_i]]*sum(1-outside_prior)).T,\n",
    "                                                loc=network_pred_mu_db[p_i].to_numpy()[~outside_prior],\n",
    "                                                scale=network_std_db[p_i].to_numpy()[~outside_prior]),c='k',alpha=0.1)\n",
    "    ax[y,x].plot(X_plot_dict[p_i],norm.pdf(np.array([X_plot_dict[p_i]]*sum(outside_prior)).T,\n",
    "                                                loc=network_pred_mu_db[p_i].to_numpy()[outside_prior],\n",
    "                                                scale=network_std_db[p_i].to_numpy()[outside_prior]),c='red',alpha=0.1)\n",
    "    #Multiplying the prior by 5 so it can be seen:\n",
    "    ax[y,x].plot(X_plot_dict[p_i],5*norm.pdf(X_plot_dict[p_i],prior_db_indx.loc[p_i,'mean'],\n",
    "                                    prior_db_indx.loc[p_i,'std']),c='blue')\n",
    "    ax[y,x].legend(handles=[\n",
    "    mpatches.Patch(color='k', label='Paltas Posterior'),\n",
    "    mpatches.Patch(color='red', label=f'Paltas Posterior ({int(np.round(100*sum(outside_prior)/len(outside_prior)))}% outside $1\\sigma$ prior)'),\n",
    "    mpatches.Patch(color='blue', label='Training Prior')],fontsize=8)\n",
    "    ax[y,x].set_xlabel(p_i)\n",
    "    ax2.scatter(np.median(network_std_db[p_i].to_numpy()),8-n_i,label=p_i)\n",
    "    ax2.scatter(prior_db_indx.loc[p_i,'std'],8-n_i,marker='x',label='_nolegend_',c='red')\n",
    "\n",
    "ax2.set_xlim(left=0)\n",
    "ax2.set_title('$\\sigma_{learnt}$ vs $\\sigma_{prior}$',fontsize=18)\n",
    "ax2.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe89d50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T13:05:35.151056Z",
     "start_time": "2023-11-15T13:05:33.349988Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata_val=pd.read_csv(training_directory+'/validation/1/metadata.csv')\n",
    "bright_source_indx = np.where(metadata_val['source_parameters_mag_app']<25)[0]\n",
    "final_epoch_n = np.max(list(network_predictions_dict.keys()))\n",
    "network_truth_db = pd.DataFrame(network_predictions_dict[final_epoch_n][0],columns=learning_params)\n",
    "network_pred_mu_db = pd.DataFrame(network_predictions_dict[final_epoch_n][1],columns=learning_params)\n",
    "network_std_db = pd.DataFrame(network_predictions_dict[final_epoch_n][2],columns=learning_params)\n",
    "\n",
    "#Assert that the true parameters from the metadata are equal to those I'm getting from gen_network_predictions, to make sure there hasn't been any reshuffling\n",
    "#and to make sure I'm comparing the properties of the same objects. The 'round' is to remove the problem of some being saved as float32 and others as float64 files.\n",
    "assert (np.round(network_truth_db[learning_params],3)==np.round(metadata_val[learning_params],3).astype('float32')).all().all()\n",
    "\n",
    "property_list = ['source_parameters_mag_app','lens_light_parameters_mag_app','main_deflector_parameters_theta_E',\n",
    "#                 'main_deflector_parameters_z_lens','lens_light_parameters_z_source','source_parameters_z_source',\n",
    "                'main_deflector_parameters_e1','main_deflector_parameters_e2']\n",
    "fig,ax = pl.subplots(len(property_list),2,figsize=(8,3*len(property_list)))\n",
    "for p_i,image_property in enumerate(property_list):\n",
    "    ax[p_i,0].scatter(\n",
    "                metadata_val[image_property],\n",
    "                network_pred_mu_db['main_deflector_parameters_theta_E']-network_truth_db['main_deflector_parameters_theta_E'],s=1)\n",
    "    ax[p_i,0].set_ylabel('Pred-Truth',fontsize=15)\n",
    "    ax[p_i,1].scatter(metadata_val[image_property],network_std_db['main_deflector_parameters_theta_E'],s=1)\n",
    "    ax[p_i,1].set_ylabel('$\\sigma_{network}$',fontsize=15)\n",
    "    for r_i in range(2): ax[p_i,r_i].set_xlabel(image_property,fontsize=10)\n",
    "\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d549e4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:12:04.106726Z",
     "start_time": "2023-11-14T16:11:58.595284Z"
    }
   },
   "outputs": [],
   "source": [
    "bins_dict = {'main_deflector_parameters_theta_E':np.arange(-3,3.5,0.5),\n",
    "'main_deflector_parameters_gamma1':np.arange(-2.5,3,0.5),\n",
    "'main_deflector_parameters_gamma2':np.arange(-2.5,3,0.5),\n",
    "'main_deflector_parameters_gamma':np.arange(-3,3.5,0.5),\n",
    "'main_deflector_parameters_e1':np.arange(-2,2.5,0.5),\n",
    "'main_deflector_parameters_e2':np.arange(-2,2.5,0.5),\n",
    "'main_deflector_parameters_center_x':np.arange(-3,3.5,0.5),\n",
    "'main_deflector_parameters_center_y':np.arange(-3,3.5,0.5),\n",
    "}\n",
    "fig,ax = pl.subplots(1,len(network_pred_mu_db.columns),figsize=(5*len(network_pred_mu_db.columns),5))\n",
    "for n_ii,c_i in enumerate(network_pred_mu_db.columns):\n",
    "    hist_dict = {'density':True,'bins':bins_dict[c_i]}\n",
    "    ax[n_ii].hist(((network_pred_mu_db[c_i]-prior_db_indx.loc[c_i,'mean'])/prior_db_indx.loc[c_i,'std']),**hist_dict)\n",
    "    ax[n_ii].hist(((network_pred_mu_db[c_i]-prior_db_indx.loc[c_i,'mean'])/prior_db_indx.loc[c_i,'std'])[bright_source_indx],fill=False,edgecolor='k',**hist_dict)\n",
    "    ax[n_ii].set_xlabel(c_i,fontsize=12)\n",
    "    ax[n_ii].set_ylabel('(Pred-$\\mu_{train}$)/$\\sigma_{train}$',fontsize=12)\n",
    "    ax[n_ii].legend(['Full','$m_{source}$<25'])\n",
    "pl.tight_layout()\n",
    "pl.show()\n",
    "\n",
    "bins_dict2 = {'main_deflector_parameters_theta_E':np.arange(-0.2,0.22,0.02),\n",
    "'main_deflector_parameters_gamma1':np.arange(-0.16,0.18,0.02),\n",
    "'main_deflector_parameters_gamma2':np.arange(-0.16,0.18,0.02),\n",
    "'main_deflector_parameters_gamma':np.arange(-0.5,0.6,0.1),\n",
    "'main_deflector_parameters_e1':np.arange(-0.16,0.18,0.02),\n",
    "'main_deflector_parameters_e2':np.arange(-0.16,0.18,0.02),\n",
    "'main_deflector_parameters_center_x':np.arange(-0.16,0.18,0.02),\n",
    "'main_deflector_parameters_center_y':np.arange(-0.16,0.18,0.02),\n",
    "}\n",
    "fig,ax = pl.subplots(1,len(network_pred_mu_db.columns),figsize=(5*len(network_pred_mu_db.columns),5))\n",
    "for n_ii,c_i in enumerate(network_pred_mu_db.columns):\n",
    "    hist_dict = {'density':True,'bins':bins_dict2[c_i]}\n",
    "    ax[n_ii].hist((network_pred_mu_db[c_i]-network_truth_db[c_i]),**hist_dict)\n",
    "    ax[n_ii].hist((network_pred_mu_db[c_i]-network_truth_db[c_i])[bright_source_indx],fill=False,edgecolor='k',**hist_dict)\n",
    "    ax[n_ii].set_xlabel(f'Pred-Truth \\n {c_i}',fontsize=12)\n",
    "    ax[n_ii].set_ylabel('Probability Density',fontsize=12)\n",
    "    ax[n_ii].legend(['Full','$m_{source}$<25'])\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad53f76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T13:26:08.026488Z",
     "start_time": "2023-11-15T13:26:02.024748Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prop_e_list = ['main_deflector_parameters_e1','main_deflector_parameters_e2']\n",
    "fig,ax=pl.subplots(1,2,figsize=(10,5))\n",
    "for prop in prop_e_list:\n",
    "    ax[0].scatter(network_truth_db[prop],network_pred_mu_db[prop],label=prop,alpha=0.5)\n",
    "    ax[0].set_xlabel('Truth');ax[0].set_ylabel('Prediction $\\mu$')\n",
    "    ax[1].scatter(network_truth_db[prop],network_std_db[prop],label=prop,alpha=0.5)\n",
    "    ax[1].set_xlabel('Truth');ax[1].set_ylabel('Prediction $\\sigma$')\n",
    "    ax[0].set_xlim(-0.4,0.4)\n",
    "    ax[0].set_ylim(-0.4,0.4)\n",
    "    ax[0].axis('equal')\n",
    "ax[0].legend()\n",
    "pl.show()\n",
    "corner.corner(network_truth_db[prop_e_list],labels=prop_e_list,range=[(-0.3,0.3),(-0.3,0.3)],\n",
    "             truths=[0,0]) #Highlighting zero-ellipticity on the plot\n",
    "pl.tight_layout()\n",
    "pl.show()\n",
    "#\n",
    "corner.corner(network_truth_db)\n",
    "pl.show()\n",
    "print(network_truth_db.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e6e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:34:45.007958Z",
     "start_time": "2023-11-14T18:34:44.988939Z"
    }
   },
   "outputs": [],
   "source": [
    "prior_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d10f5e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Plot Network Output Distributions\n",
    "We now plot the distributions of the network predictions, and compare those to the ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79967c05",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "db_columns = [elem.replace('main_deflector_parameters_','') for elem in learning_params]\n",
    "Error_db = {elem:pd.DataFrame(columns=db_columns) for elem in network_predictions_dict.keys()}\n",
    "\n",
    "def RMS_error_func(pred,truth,db_columns):\n",
    "    #Calculates RMS error\n",
    "    RMS_error =np.sqrt(np.mean((pred-truth)**2,axis=0))\n",
    "    print(f'{len(RMS_error)} dimensional output') \n",
    "    return {db_columns[i]:RMS_error[i] for i in range(len(db_columns))}\n",
    "\n",
    "def MAE_func(pred,truth,db_columns):\n",
    "    #Calculates mean absolute error\n",
    "    MAE_error = np.mean(abs(pred-truth),axis=0) \n",
    "    print(f'{len(MAE_error)} dimensional output') \n",
    "    return {db_columns[i]:MAE_error[i] for i in range(len(db_columns))}\n",
    "\n",
    "#RMS_error_func(network_predictions_dict[epoch_i][0],network_predictions_dict[epoch_i][1])\n",
    "\n",
    "for epoch_i in network_predictions_dict.keys():\n",
    "    Error_db[epoch_i] = pd.concat([Error_db[epoch_i],pd.DataFrame(data=MAE_func(network_predictions_dict[epoch_i][0],\n",
    "                                                                                network_predictions_dict[epoch_i][1],\n",
    "                                                                                db_columns),\n",
    "                                                                index=np.array(['MAE']))])\n",
    "    Error_db[epoch_i] = pd.concat([Error_db[epoch_i],pd.DataFrame(data=RMS_error_func(network_predictions_dict[epoch_i][0],\n",
    "                                                                                      network_predictions_dict[epoch_i][1],\n",
    "                                                                                      db_columns),\n",
    "                                                                index=np.array(['RMS']))])\n",
    "\n",
    "Error_db[epoch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9575c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T00:19:17.305895Z",
     "start_time": "2023-08-21T00:19:10.361558Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "label_kwargs = {'fontsize': 20}\n",
    "range_dict = {'main_deflector_parameters_theta_E':(0,3),\n",
    "              'main_deflector_parameters_gamma':(1,3),\n",
    "              'main_deflector_parameters_gamma1':(-0.5,0.5),\n",
    "              'main_deflector_parameters_gamma2':(-0.5,0.5),\n",
    "              'main_deflector_parameters_e1':(-0.5,0.5),\n",
    "              'main_deflector_parameters_e2':(-0.5,0.5),\n",
    "              'main_deflector_parameters_center_x':(-0.5,0.5),\n",
    "              'main_deflector_parameters_center_y':(-0.5,0.5)}\n",
    "bins_corner=20\n",
    "gif_images = []\n",
    "for epoch_i in tqdm(network_predictions_dict.keys()):\n",
    "    fig = pl.figure(figsize=(3*len(learning_params),3*len(learning_params)))\n",
    "    corner_kwargs_dict = {'fig':fig,'bins':bins_corner,'range':[range_dict[elem] for elem in learning_params]}\n",
    "    corner.corner(network_predictions_dict[epoch_i][0],color='k',**corner_kwargs_dict)\n",
    "    corner.corner(network_predictions_dict[epoch_i][1],color='red',\\\n",
    "                labels=['$'+elem+'$' for elem in corner_param_print],\\\n",
    "                label_kwargs=label_kwargs,\n",
    "                **corner_kwargs_dict)\n",
    "    pl.legend(['Truth','Pred'])\n",
    "    pl.tight_layout()\n",
    "#To save as a gif:\n",
    "    pl.suptitle(f'Epoch {epoch_i}',fontsize=25,fontweight='bold')\n",
    "    try:\n",
    "        pl.savefig(f'{model_directory}/corner_plots/corner_plot_evolution_{epoch_i}.png')\n",
    "    except:\n",
    "        os.mkdir(f'{model_directory}/corner_plots/')\n",
    "        pl.savefig(f'{model_directory}/corner_plots/corner_plot_evolution_{epoch_i}.png')\n",
    "#    pl.show()\n",
    "    pl.close()\n",
    "    corner_i = imageio.imread(f'{model_directory}/corner_plots/corner_plot_evolution_{epoch_i}.png')\n",
    "    gif_images.append(corner_i)\n",
    "\n",
    "imageio.mimsave(f'{model_directory}/corner_plots/corner_plot_evolution.gif', gif_images,duration=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69abca-7ed4-431b-adda-fe3bb2402030",
   "metadata": {},
   "source": [
    "## Load Model Outputs\n",
    "The hyperparameters of the training set are loaded (to use as an interim prior in the hierarchical inference), along with the network predictions for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d5996-53bc-45cd-a958-e12b338ec918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T13:43:27.504365Z",
     "start_time": "2023-11-14T13:43:26.643704Z"
    }
   },
   "outputs": [],
   "source": [
    "train_mean = np.array(pd.read_csv(glob.glob(f'{training_directory}/**/norm*',recursive=True)[0])['mean']) \n",
    "train_scatter = np.array(pd.read_csv(glob.glob(f'{training_directory}/**/norm*',recursive=True)[0])['std']) \n",
    "\n",
    "#Since we are using a diagonal covariance matrix, the precision matrix is the diagonal matrix of\n",
    "#the (elementwise) values of 1/std^2. In general however it is inv(cov_matrix).\n",
    "final_epoch = max(list(network_predictions_dict.keys()))\n",
    "network_means = network_predictions_dict[final_epoch][1][:,:].astype('float64')              \n",
    "network_prec = network_predictions_dict[final_epoch][3][:,:,:].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(network_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1388d8f2-b579-4838-8c6b-22eefdc07818",
   "metadata": {},
   "source": [
    "# Hierarchical Inference\n",
    "The following performs hierarchical inference to retrieve the population hyperparameters of the validation set, assuming a diagonal covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698364e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:27:36.618660Z",
     "start_time": "2023-11-14T18:27:36.609479Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_params[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96032fb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T12:56:16.446371Z",
     "start_time": "2023-11-15T12:56:16.433573Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(np.isnan(chain_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1af76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T13:23:16.627397Z",
     "start_time": "2023-11-15T13:23:11.766353Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "def plot_sampler_properties(plot_evolution=False,plot_post_burnin=False,burnin=1000,burnout=None,\n",
    "                           learning_params_for_HI=None):\n",
    "    warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "    #[walker,evolution_number,property]\n",
    "    N_cols = len(learning_params_for_HI) #Needs to stay as N_params, as one row is the mean, the other is the sigma (and the titles are defined as such.)\n",
    "    N_rows = np.ceil(2*len(learning_params_for_HI)/N_cols).astype('int')\n",
    "    print(N_cols,N_rows)\n",
    "    if plot_evolution:\n",
    "        fig,ax = pl.subplots(N_rows,N_cols,figsize=(5*N_cols,5*N_rows))\n",
    "        for prop_i in range(2*len(learning_params_for_HI)):\n",
    "            x = prop_i%N_cols\n",
    "            y = np.floor(prop_i/N_cols).astype('int')\n",
    "            param_i = learning_params_for_HI[prop_i%len(learning_params_for_HI)]\n",
    "            for c_i in range(40):\n",
    "                ax[y,x].plot(chain_orig[c_i,:,prop_i],alpha=0.1,c='k')\n",
    "                ax[y,x].set_title('$\\mu$ ('*(y==0)+'$\\sigma$ ('*(y==1)+learning_params_for_HI[prop_i%N_cols].replace('main_deflector_parameters_','')+')',\n",
    "                                fontsize=16)\n",
    "                ax[y,x].set_xlim(burnin,burnout)\n",
    "            if prop_i<len(learning_params_for_HI):\n",
    "                    ax[y,x].plot(ax[y,x].get_xlim(),\n",
    "                                 [prior_db_indx.loc[param_i]['mean'],\n",
    "                                  prior_db_indx.loc[param_i]['mean']],\n",
    "                                 c='red',label='prior mean',linewidth=5)\n",
    "            else:\n",
    "                    ax[y,x].plot(ax[y,x].get_xlim(),\n",
    "                                 np.log(\n",
    "                                 [prior_db_indx.loc[param_i]['std'],\n",
    "                                  prior_db_indx.loc[param_i]['std']]),\n",
    "                                 c='red',label='prior std',linewidth=5)\n",
    "            ax[y,x].legend(loc='lower right')\n",
    "#            ax[y,x].set_ylim(-1,1)\n",
    "        pl.tight_layout()\n",
    "        pl.show()\n",
    "    if plot_post_burnin:\n",
    "        print(\"NOTE: The sigma are in natural logarithms, not in log10.\")\n",
    "        fig,ax = pl.subplots(N_rows,N_cols,figsize=(5*N_cols,5*N_rows))\n",
    "        for prop_i in range(2*len(learning_params_for_HI)):\n",
    "            prop_i_val = learning_params_for_HI[prop_i%len(learning_params_for_HI)]\n",
    "            x = prop_i%N_cols\n",
    "            y = np.floor(prop_i/N_cols).astype('int')\n",
    "            if prop_i<len(learning_params_for_HI):\n",
    "                bins=np.linspace(\n",
    "                             prior_db_indx.loc[prop_i_val]['mean']-prior_db_indx.loc[prop_i_val]['std'],\n",
    "                             prior_db_indx.loc[prop_i_val]['mean']+prior_db_indx.loc[prop_i_val]['std'],\n",
    "                             50)\n",
    "                ax[y,x].hist(chain_orig[:,burnin:burnout,prop_i].flatten(),bins=bins)\n",
    "                ax_ylim = ax[y,x].get_ylim()\n",
    "                ax[y,x].plot([prior_db_indx.loc[prop_i_val]['mean']]*2,ax_ylim)\n",
    "            else:\n",
    "                bins=50\n",
    "                ax[y,x].hist(chain_orig[:,burnin:burnout,prop_i].flatten(),bins=bins)\n",
    "                ax_ylim = ax[y,x].get_ylim()\n",
    "                ax[y,x].plot(np.log([prior_db_indx.loc[prop_i_val]['std']]*2),ax_ylim)\n",
    "            ax[y,x].set_title('$\\mu$ ('*(y==0)+'$\\sigma$ ('*(y==1)+learning_params_for_HI[prop_i%N_cols].replace('main_deflector_parameters_','')+')')\n",
    "        pl.tight_layout()\n",
    "        pl.show()\n",
    "\n",
    "plot_sampler_properties(plot_evolution=True,\n",
    "                        plot_post_burnin=True,\n",
    "                        burnin=1000,burnout=None,\n",
    "                        learning_params_for_HI = ['main_deflector_parameters_theta_E',\n",
    "                                                  'main_deflector_parameters_gamma',\n",
    "#                                                 'main_deflector_parameters_gamma1',\n",
    "#                                                 'main_deflector_parameters_gamma2'])\n",
    "#                                                  'main_deflector_parameters_center_x',\n",
    "#                                                  'main_deflector_parameters_center_y',\n",
    "#                                                  'main_deflector_parameters_e1'])\n",
    "                                                  'main_deflector_parameters_e2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+\n",
    "hierarchical_inference = reload(hierarchical_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e536dcd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:14:25.725683Z",
     "start_time": "2023-11-14T16:14:25.096299Z"
    }
   },
   "outputs": [],
   "source": [
    "#Due to the current implementation of hierarchical_inference.ProbabilityClassAnalytical.log_post_omega, we assert\n",
    "#that the covariance matrix must be diagonal for now.\n",
    "assert loss_function=='diag'\n",
    "burnin = int(1000)\n",
    "n_samps = 3000\n",
    "#Saving arrays for hierarchical inference:\n",
    "try:\n",
    "    os.mkdir(f'{model_directory}/mcmc_files/')\n",
    "except:\n",
    "    pass\n",
    "np.save(model_directory+'/mcmc_files/network_means.npy',network_means)\n",
    "np.save(model_directory+'/mcmc_files/network_prec.npy',network_prec)\n",
    "np.save(model_directory+'/mcmc_files/train_mean.npy',train_mean)\n",
    "np.save(model_directory+'/mcmc_files/train_scatter.npy',train_scatter)\n",
    "np.save(model_directory+'/mcmc_files/learning_params.npy',learning_params)\n",
    "prior_db_indx.to_csv(model_directory+'/mcmc_files/prior_db_indx.csv')\n",
    "\n",
    "#The MCMC hierarchical inference now takes place in ./run_mcmc.py:\n",
    "print(f\"addqueue -c '45min' -m  3 /mnt/users/hollowayp/python11_env/bin/python3.11 ./run_mcmc.py {model_directory} {n_samps} {num_params}\")\n",
    "!addqueue -c '45min' -m  3 /mnt/users/hollowayp/python11_env/bin/python3.11 ./run_mcmc.py $model_directory $n_samps $num_params\n",
    "print('Note: run_mcmc assumes there are 1000 lenses in the test set - otherwise the reshaping will be wrong.')\n",
    "print('Settings in run_mcmc.py:'+\\\n",
    "        '\\n1) The mu values must be within 5-sigma of the training prior. The mu and sigma values are HARDCODED and need to be UPDATED with each new training set.'+\\\n",
    "        '\\n2) The sigma values must be between 0.001 and 2. (natural-logged, in np.log space)'+\\\n",
    "        '\\n3) The mu walkers are initialised within 5 sigma of the training prior'+\\\n",
    "        '\\n4) The sigma walkers are initialsed between 1% of the sigma and 3-sigma.')\n",
    "\n",
    "print(\n",
    "    '\\n A few notes on MCMC:'+\\\n",
    "    '\\n 1) I think this is broadly evaluating C5 in the appendix of Wagner Carena 2021. I think the \"normalising factor\" terms in C6 '+\\\n",
    "        'are ignored? Or, it is evaluating C6, not integrating at all, just taking the product of the 3 gaussians?? But gaus_prod_analytical does look like an integral'+\\\n",
    "    '\\n 2) mu_omega_i: I think this comes from the mean of the parameters in the training set, i.e. train_mean above. '+\\\n",
    "        'This acts as the prior.'\n",
    "    '\\n 3) mu_omega: I think this is updated during the MCMC sampling. It starts out as cur_state_mu, defined above, then evolves over'+\\\n",
    "        'time. Note that p(omega|{d}) $\\propto$ p(xi_i|Omega)...extraterms, so the equation must be solved implicitly. I.e. omega'+\\\n",
    "        'appears on both sides. I think the MCMC evolves the values of omega, sampling the parameter space.'\n",
    "    '\\n 4) mu_pred: I think these are the predictions from the network, dependent on the particular image being considered. The '+\\\n",
    "        'network outputs a mean and an uncertainty. mu_pred is the list of all the means.'+\\\n",
    "    '\\n 5) Have also set eval_func_omega to return 0, not the sum of the hyperparameters.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12005eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:00:36.639886Z",
     "start_time": "2023-11-15T10:00:36.629647Z"
    }
   },
   "outputs": [],
   "source": [
    "list(prior_db['parameter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a619619",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:32:39.077975Z",
     "start_time": "2023-11-15T10:32:39.064678Z"
    }
   },
   "outputs": [],
   "source": [
    "glob.glob(f'{model_directory}/mcmc_files/mcmc_chains*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6625df-1c23-4815-8765-d5f1ea9e6340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T13:22:18.714041Z",
     "start_time": "2023-11-15T13:22:18.497906Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading up the latest mcmc chain:\n",
    "latest_chain = np.max([int(elem.split('mcmc_chains_')[1].replace('.npy','')) for elem in glob.glob(f'{model_directory}/mcmc_files/mcmc_chains*.npy')])\n",
    "#Loading the complete chains:\n",
    "latest_chain_filename = f'{model_directory}/mcmc_files/mcmc_chains_{latest_chain}.npy'\n",
    "#latest_chain_filename=f'{model_directory}/mcmc_files/mcmc_chains_1700045646.npy'\n",
    "print(f'Loading {latest_chain_filename}, last modified at {datetime.datetime.utcfromtimestamp(os.path.getmtime(latest_chain_filename))}')\n",
    "chain_orig = np.load(latest_chain_filename) #[walker,step_number,parameters_x2]\n",
    "#Removing the burn-in:\n",
    "burnin=1000\n",
    "chain = chain_orig[:,burnin:,:].reshape((-1,2*num_params))[:,0:num_params] #Just using the means for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd52c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting loss evolution:\n",
    "def plot_loss_evolution(directory_to_save_model):\n",
    "    loss_db = pd.read_csv(directory_to_save_model+'/loss_function_db.csv')\n",
    "    pl.plot(loss_db['epoch'],loss_db['loss'],c='k')\n",
    "    pl.plot(loss_db['epoch'],loss_db['val_loss'],c='blue')\n",
    "    pl.xlabel('Epoch',fontsize=12)\n",
    "    pl.ylabel('Loss',fontsize=12)\n",
    "    pl.legend(['Training Loss','Validation Loss'])\n",
    "    pl.tick_params(labelsize=10)\n",
    "    pl.tight_layout()\n",
    "    pl.ylim(-3,5)\n",
    "    pl.sh ow()\n",
    "\n",
    "plot_loss_evolution(model_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eee64d8-1ef0-45f5-8236-45b847d4b87b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results\n",
    "We plot the results of the hierarchical inference here: the ground-truth is plotted as solid black lines, along with the population posterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787341a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T03:15:54.173143Z",
     "start_time": "2023-08-21T03:15:48.438905Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_kwargs = {'fontsize':20}\n",
    "hist_kwargs = {'density':False,'color':'orange','lw':3}\n",
    "fig,ax = pl.subplots(len(learning_params),len(learning_params),figsize=(3*len(learning_params),3*len(learning_params)))\n",
    "\n",
    "f = 10 #Bounds of corner plot, in units of sigma\n",
    "\n",
    "corner.corner(chain,\n",
    "                labels=['$\\mu_{'+elem+'}$' for elem in corner_param_print],\n",
    "                range=[(prior_db_indx.loc[elem]['mean']-f*prior_db_indx.loc[elem]['std'],\n",
    "                        prior_db_indx.loc[elem]['mean']+f*prior_db_indx.loc[elem]['std']) for elem in learning_params],\n",
    "                fig=fig,\n",
    "                show_titles=False,\n",
    "                plot_datapoints=True,\n",
    "                label_kwargs=labels_kwargs,\\\n",
    "                levels=[0.68,0.95],\n",
    "                color='orange',\n",
    "                fill_contours=True,\n",
    "                hist_kwargs=hist_kwargs,\n",
    "                title_fmt='.2f',\\\n",
    "                #truths=[1.1,0,0,0,0,0,0],\n",
    "                truth_color='k',\\\n",
    "                max_n_ticks=3,\n",
    "                bins=50,\n",
    "                s=100\n",
    "                )\n",
    "\n",
    "sig_learning_params = np.array(prior_db['std'])#np.array([0.15,0.05,0.05,0.1,0.1,0.1,0.16,0.16])\n",
    "loc_learning_params = np.array(prior_db['mean'])#np.array([1.1,0,0,2,0,0,0,0])\n",
    "for i in range(len(learning_params)):\n",
    "    for j in range(len(learning_params)):\n",
    "        ax[i,j].set_xlim((loc_learning_params-f*sig_learning_params)[j],(loc_learning_params+f*sig_learning_params)[j])\n",
    "        if i!=j:\n",
    "            ax[i,j].set_ylim((loc_learning_params-f*sig_learning_params)[i],(loc_learning_params+f*sig_learning_params)[i])\n",
    "\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484cc85",
   "metadata": {},
   "source": [
    "# Endnote\n",
    "We have now 1) generated simulated lensed images, 2) trained a neural network to model these images, 3) applied the trained network to a different set of images and 4) run hierarchical inference to infer population-level parameters.\\\n",
    "You may note that the final results could be improved - the configuration settings used here were chosen for speed rather than precision. For science-level results, the following should be changed:\n",
    "1) Increase the training-set size for the neural network,\n",
    "2) Increase the number of epochs the network trains for,\n",
    "3) Increase the number of iterations (and burn-in) of the MCMC used for hierarchical inference.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "685px",
    "left": "28px",
    "top": "359px",
    "width": "305.74px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
